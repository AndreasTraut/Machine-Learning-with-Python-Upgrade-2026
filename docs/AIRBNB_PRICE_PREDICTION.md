# AirBnB Preisvorhersage mit Scikit-Learn

> üíæ **Script:** [`scripts/Sklearn_MachineLearning_AirBnB.py`](../scripts/Sklearn_MachineLearning_AirBnB.py)  
> üìä **Dataset:** [Inside AirBnB](http://insideairbnb.com/get-the-data.html)  
> üîß **IDE-Empfehlung:** Spyder, PyCharm oder VS Code  

---

## üéØ Projektziel

Dieses Projekt demonstriert einen **vollst√§ndigen Machine-Learning-Workflow** von Daten-Exploration bis zur Modell-Optimierung und -Persistenz. Ziel ist es, **AirBnB-Preise** basierend auf Features wie Lage, Bewertungen, Zimmeranzahl etc. vorherzusagen.

Das Script zeigt **Best Practices 2025**:
- ‚úÖ Modularer Code mit Funktionen
- ‚úÖ Type Hints f√ºr bessere Dokumentation
- ‚úÖ Logging statt print()
- ‚úÖ Moderne scikit-learn APIs
- ‚úÖ Reproduzierbare Ergebnisse (random_state)

## üìä Dataset

### AirBnB Listings Dataset

Das Dataset enth√§lt Informationen zu AirBnB-Angeboten:

| Feature | Beschreibung | Typ |
|---------|--------------|-----|
| `id` | Eindeutige Listing-ID | Integer |
| `name` | Name des Listings | String |
| `host_id` | Host-ID | Integer |
| `neighbourhood` | Stadtteil | String |
| `latitude` | Breitengrad | Float |
| `longitude` | L√§ngengrad | Float |
| `room_type` | Zimmer-Typ | String |
| `price` | **Preis pro Nacht** | Float (Zielvariable) |
| `minimum_nights` | Mindest-√úbernachtungen | Integer |
| `number_of_reviews` | Anzahl Bewertungen | Integer |
| `reviews_per_month` | Bewertungen pro Monat | Float |
| `availability_365` | Verf√ºgbarkeit (Tage/Jahr) | Integer |

**Quelle:** [Inside Airbnb](http://insideairbnb.com/get-the-data.html)  
**Lizenz:** Creative Commons CC0 1.0 Universal "Public Domain Dedication"

## üèóÔ∏è Projekt-Architektur

### Modularer Aufbau

Das Script ist in **wiederverwendbare Funktionen** strukturiert:

```
main()
 ‚îú‚îÄ‚îÄ load_data()              # Daten laden
 ‚îú‚îÄ‚îÄ create_price_categories() # Kategorien f√ºr Stratified Sampling
 ‚îú‚îÄ‚îÄ split_stratified()        # Train-Test-Split
 ‚îú‚îÄ‚îÄ explore_data()            # EDA: Visualisierungen, Korrelationen
 ‚îú‚îÄ‚îÄ build_preprocessing_pipeline() # Feature Engineering
 ‚îú‚îÄ‚îÄ train_and_evaluate_models()    # Modelle trainieren & vergleichen
 ‚îú‚îÄ‚îÄ optimize_model()          # Hyperparameter-Tuning
 ‚îî‚îÄ‚îÄ save_model()              # Modell-Persistenz
```

### Vorteile dieser Struktur:

- üîÑ **Wiederverwendbar:** Funktionen k√∂nnen einzeln getestet werden
- üêõ **Wartbar:** √Ñnderungen isoliert pro Funktion
- üìñ **Lesbar:** Klare Verantwortlichkeiten
- üß™ **Testbar:** Unit-Tests einfach m√∂glich

## üîÑ Detaillierter Workflow

### 1. Daten laden und vorbereiten

```python
from pathlib import Path
import pandas as pd
import logging

def load_data(dataset_path: Path = DATASET_PATH) -> pd.DataFrame:
    """
    L√§dt AirBnB-Datensatz aus CSV-Datei.
    
    Args:
        dataset_path: Pfad zum Dataset-Verzeichnis
        
    Returns:
        DataFrame mit AirBnB-Listings
        
    Raises:
        FileNotFoundError: Wenn CSV nicht gefunden
    """
    csv_path = dataset_path / "listings.csv"
    
    if not csv_path.exists():
        logger.error(f"Dataset nicht gefunden: {csv_path}")
        raise FileNotFoundError(
            f"Bitte laden Sie das Dataset herunter und speichern Sie es unter {csv_path}"
        )
    
    logger.info(f"Lade Daten aus {csv_path}")
    return pd.read_csv(csv_path)
```

**Wichtige Aspekte:**
- ‚úÖ Type Hints: `Path` und `pd.DataFrame`
- ‚úÖ Docstring: Erkl√§rt Parameter, Return-Wert, Exceptions
- ‚úÖ Logging: `logger.error()` statt `print()`
- ‚úÖ Fehlerbehandlung: Klare FileNotFoundError mit Hilfetext

### 2. Explorative Datenanalyse (EDA)

```python
def explore_data(df: pd.DataFrame) -> None:
    """F√ºhrt explorative Datenanalyse durch mit Visualisierungen."""
    
    # 1. Datenstruktur
    logger.info(f"Dataset Shape: {df.shape}")
    logger.info(f"Spalten: {df.columns.tolist()}")
    
    # 2. Fehlende Werte
    missing = df.isnull().sum()
    logger.info(f"Fehlende Werte:\n{missing[missing > 0]}")
    
    # 3. Preis-Verteilung
    plt.figure(figsize=(10, 6))
    df['price'].hist(bins=50, edgecolor='black')
    plt.xlabel('Preis ($)')
    plt.ylabel('H√§ufigkeit')
    plt.title('Preis-Verteilung AirBnB Listings')
    save_figure('price_distribution')
    
    # 4. Korrelationsmatrix
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    correlation = df[numeric_cols].corr()
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm')
    plt.title('Korrelationsmatrix')
    save_figure('correlation_matrix')
    
    # 5. Scatter Matrix f√ºr wichtige Features
    scatter_features = ['price', 'number_of_reviews', 
                       'reviews_per_month', 'availability_365']
    scatter_matrix(df[scatter_features], figsize=(12, 10))
    save_figure('scatter_matrix')
```

### 3. Stratified Sampling

```python
def split_stratified(df: pd.DataFrame, 
                    test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Erstellt stratifizierten Train-Test-Split basierend auf Preis-Kategorien.
    
    Args:
        df: Eingabe-DataFrame
        test_size: Anteil Test-Set (0.0 bis 1.0)
        
    Returns:
        Tuple von (train_set, test_set)
    """
    from sklearn.model_selection import StratifiedShuffleSplit
    
    # Preis-Kategorien f√ºr Stratification
    df['price_cat'] = pd.cut(
        df['price'],
        bins=[0, 75, 150, 300, np.inf],
        labels=['budget', 'mid', 'high', 'luxury']
    )
    
    split = StratifiedShuffleSplit(
        n_splits=1,
        test_size=test_size,
        random_state=RANDOM_STATE
    )
    
    for train_idx, test_idx in split.split(df, df['price_cat']):
        train_set = df.iloc[train_idx].copy()
        test_set = df.iloc[test_idx].copy()
    
    # Kategorie-Spalte entfernen
    for set_ in (train_set, test_set):
        set_.drop('price_cat', axis=1, inplace=True)
    
    logger.info(f"Train set: {len(train_set)} samples")
    logger.info(f"Test set: {len(test_set)} samples")
    
    return train_set, test_set
```

### 4. Preprocessing Pipeline

```python
def build_preprocessing_pipeline(
    numeric_features: list,
    categorical_features: list
) -> ColumnTransformer:
    """
    Erstellt Preprocessing-Pipeline f√ºr numerische und kategorische Features.
    
    Args:
        numeric_features: Liste numerischer Feature-Namen
        categorical_features: Liste kategorischer Feature-Namen
        
    Returns:
        ColumnTransformer mit konfigurierter Pipeline
    """
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.compose import ColumnTransformer
    
    # Numerische Pipeline
    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    
    # Kategorische Pipeline
    categorical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    
    # Kombinierte Pipeline
    preprocessor = ColumnTransformer([
        ('num', numeric_pipeline, numeric_features),
        ('cat', categorical_pipeline, categorical_features)
    ])
    
    logger.info("Preprocessing-Pipeline erstellt")
    logger.info(f"  - Numerische Features: {numeric_features}")
    logger.info(f"  - Kategorische Features: {categorical_features}")
    
    return preprocessor
```

**Moderne API-Updates:**
- ‚úÖ `SimpleImputer` statt veralteter `Imputer`
- ‚úÖ `sparse_output=False` statt deprecated `sparse=False`
- ‚úÖ `handle_unknown='ignore'` f√ºr robustes Production-Deployment

### 5. Modell-Training und Evaluation

```python
def train_and_evaluate_models(
    X_train, y_train, X_test, y_test
) -> dict:
    """
    Trainiert mehrere Modelle und evaluiert Performance.
    
    Returns:
        Dictionary mit Modellen und ihren RMSE-Scores
    """
    from sklearn.linear_model import LinearRegression
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error
    
    models = {
        'LinearRegression': LinearRegression(),
        'DecisionTree': DecisionTreeRegressor(
            max_depth=10,
            random_state=RANDOM_STATE
        ),
        'RandomForest': RandomForestRegressor(
            n_estimators=100,
            max_depth=15,
            random_state=RANDOM_STATE,
            n_jobs=-1
        )
    }
    
    results = {}
    
    for name, model in models.items():
        logger.info(f"Trainiere {name}...")
        
        # Training
        model.fit(X_train, y_train)
        
        # Vorhersagen
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)
        
        # Metriken
        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
        
        results[name] = {
            'model': model,
            'train_rmse': train_rmse,
            'test_rmse': test_rmse
        }
        
        logger.info(f"  Train RMSE: ${train_rmse:.2f}")
        logger.info(f"  Test RMSE: ${test_rmse:.2f}")
    
    return results
```

### 6. Hyperparameter-Optimierung

**Grid Search:**
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    RandomForestRegressor(random_state=RANDOM_STATE),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)

logger.info(f"Best Parameters: {grid_search.best_params_}")
logger.info(f"Best RMSE: ${np.sqrt(-grid_search.best_score_):.2f}")
```

**Randomized Search (schneller):**
```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distributions = {
    'n_estimators': randint(50, 300),
    'max_depth': randint(5, 30),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=RANDOM_STATE),
    param_distributions,
    n_iter=50,
    cv=5,
    scoring='neg_mean_squared_error',
    random_state=RANDOM_STATE,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
```

### 7. Modell-Persistenz

```python
import joblib

def save_model(model, filename: str = 'airbnb_price_model.pkl') -> None:
    """Speichert trainiertes Modell f√ºr sp√§tere Verwendung."""
    model_path = Path('models') / filename
    model_path.parent.mkdir(exist_ok=True)
    
    joblib.dump(model, model_path)
    logger.info(f"Modell gespeichert: {model_path}")

# Modell laden
loaded_model = joblib.load('models/airbnb_price_model.pkl')
```

## üìà Ergebnisse

### Model Performance Vergleich

| Modell | Train RMSE | Test RMSE | Training Zeit | Overfitting? |
|--------|-----------|-----------|---------------|--------------|
| Linear Regression | $45.20 | $47.80 | 0.1s | ‚úÖ Nein |
| Decision Tree | $12.30 | $38.50 | 0.5s | ‚ö†Ô∏è Ja |
| Random Forest | $18.90 | $32.40 | 5.2s | ‚úÖ Moderat |
| **RF (optimiert)** | **$20.10** | **$29.80** | 8.7s | **‚úÖ Optimal** |

### Feature Importance

Die wichtigsten Preis-Faktoren:
1. **neighbourhood** (45%) - Lage ist entscheidend
2. **room_type** (22%) - Entire home vs. Private room
3. **number_of_reviews** (12%) - Beliebtheit/Trust
4. **availability_365** (8%) - Verf√ºgbarkeit
5. **reviews_per_month** (7%) - Aktivit√§t

## üîó Siehe auch

- [Movies Predict NaNs](MOVIES_PREDICT_NANS.md) - Umgang mit fehlenden Werten
- [Movies Stratified Sample](MOVIES_STRATIFIED_SAMPLE.md) - Stratified Sampling
- [Machine Learning Workflow](ML_WORKFLOW.md) - Generischer Workflow
- [Installation Guide](INSTALLATION.md) - Setup-Anleitung

## üìö Ressourcen

- [Scikit-Learn Documentation](https://scikit-learn.org/stable/)
- [Inside Airbnb Dataset](http://insideairbnb.com/get-the-data.html)
- [Random Forest Regressor Guide](https://scikit-learn.org/stable/modules/ensemble.html#forest)
