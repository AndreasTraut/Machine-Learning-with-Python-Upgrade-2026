{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies Machine Learning - Stratified Sample (Upgrade 2025)\n",
    "\n",
    "**Autor:** Andreas Traut  \n",
    "**Datum:** Dezember 2025  \n",
    "**Version:** 2025.1\n",
    "\n",
    "## Ziel\n",
    "\n",
    "Dieses Notebook demonstriert die Verwendung von **Stratified Sampling** f√ºr ausgewogene Train-Test-Splits beim Machine Learning.\n",
    "\n",
    "## Aktualisierungen (2025)\n",
    "\n",
    "- ‚úÖ Python 3.10+ kompatibel\n",
    "- ‚úÖ scikit-learn >= 1.2 APIs\n",
    "- ‚úÖ SimpleImputer statt deprecated Imputer\n",
    "- ‚úÖ OneHotEncoder mit `handle_unknown='ignore'`\n",
    "- ‚úÖ ColumnTransformer f√ºr Feature-Typen\n",
    "- ‚úÖ StandardScaler in Pipeline\n",
    "- ‚úÖ random_state f√ºr Reproduzierbarkeit\n",
    "- ‚úÖ GridSearchCV und RandomizedSearchCV Beispiele\n",
    "\n",
    "## Anforderungen\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn jupyterlab scipy\n",
    "```\n",
    "\n",
    "## Datenquelle\n",
    "\n",
    "**Kaggle:** [IMDB 10000+ Movies Dataset](https://www.kaggle.com/datasets)\n",
    "\n",
    "Bitte laden Sie die Daten herunter und speichern Sie sie unter: `datasets/movies/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard-Bibliotheken\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Science Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedShuffleSplit,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import randint\n",
    "import joblib\n",
    "\n",
    "# Konfiguration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualisierung\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Alle Bibliotheken erfolgreich importiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad zum Dataset\n",
    "data_path = Path('../../datasets/movies/movies.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(\"‚ö†Ô∏è Dataset nicht gefunden!\")\n",
    "    print(f\"Bitte laden Sie die Daten herunter und speichern Sie sie unter: {data_path}\")\n",
    "    print(\"Quelle: https://www.kaggle.com/datasets\")\n",
    "else:\n",
    "    # Daten laden\n",
    "    movies_df = pd.read_csv(data_path)\n",
    "    print(f\"‚úÖ Daten geladen: {movies_df.shape[0]} Zeilen, {movies_df.shape[1]} Spalten\")\n",
    "    \n",
    "    # Erste Zeilen anzeigen\n",
    "    display(movies_df.head())\n",
    "    \n",
    "    # Info √ºber Dataset\n",
    "    print(\"\\nüìä Dataset Info:\")\n",
    "    movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified Sampling vorbereiten\n",
    "\n",
    "Beim **Stratified Sampling** wird sichergestellt, dass die Verteilung wichtiger Kategorien im Training- und Test-Set repr√§sentativ bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Kategorien f√ºr Revenue (falls vorhanden)\n",
    "if 'Revenue' in movies_df.columns:\n",
    "    # Entferne Zeilen ohne Revenue f√ºr dieses Beispiel\n",
    "    movies_clean = movies_df[movies_df['Revenue'].notna()].copy()\n",
    "    \n",
    "    # Erstelle Revenue-Kategorien\n",
    "    movies_clean['revenue_cat'] = pd.cut(\n",
    "        movies_clean['Revenue'],\n",
    "        bins=[0, 50, 100, 200, np.inf],\n",
    "        labels=['low', 'medium', 'high', 'very_high']\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ {len(movies_clean)} Filme mit Revenue-Werten\")\n",
    "    print(\"\\nüìä Revenue-Kategorien Verteilung:\")\n",
    "    print(movies_clean['revenue_cat'].value_counts(normalize=True))\n",
    "    \n",
    "    # Visualisierung\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    movies_clean['Revenue'].hist(bins=50, edgecolor='black')\n",
    "    plt.xlabel('Revenue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Revenue Distribution')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    movies_clean['revenue_cat'].value_counts().plot(kind='bar')\n",
    "    plt.xlabel('Revenue Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Revenue Categories')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vergleich: Random vs. Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'movies_clean' in locals():\n",
    "    # 1. Random Split\n",
    "    train_random, test_random = train_test_split(\n",
    "        movies_clean,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. Stratified Split\n",
    "    splitter = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for train_idx, test_idx in splitter.split(movies_clean, movies_clean['revenue_cat']):\n",
    "        train_stratified = movies_clean.loc[train_idx]\n",
    "        test_stratified = movies_clean.loc[test_idx]\n",
    "    \n",
    "    # Vergleiche Verteilungen\n",
    "    def compare_distributions(overall, random, stratified, category='revenue_cat'):\n",
    "        comparison = pd.DataFrame({\n",
    "            'Overall': overall[category].value_counts(normalize=True),\n",
    "            'Random': random[category].value_counts(normalize=True),\n",
    "            'Stratified': stratified[category].value_counts(normalize=True)\n",
    "        })\n",
    "        \n",
    "        comparison['Random_Error_%'] = 100 * (comparison['Random'] - comparison['Overall']) / comparison['Overall']\n",
    "        comparison['Stratified_Error_%'] = 100 * (comparison['Stratified'] - comparison['Overall']) / comparison['Overall']\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    comparison = compare_distributions(movies_clean, test_random, test_stratified)\n",
    "    \n",
    "    print(\"üìä Vergleich der Verteilungen:\")\n",
    "    display(comparison)\n",
    "    \n",
    "    print(\"\\nüí° Interpretation:\")\n",
    "    print(\"- Kleinere Fehler% = bessere Repr√§sentation\")\n",
    "    print(\"- Stratified Split hat typischerweise kleinere Fehler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Features und Pipeline vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_stratified' in locals():\n",
    "    # Entferne tempor√§re Kategorie-Spalte\n",
    "    train_set = train_stratified.drop('revenue_cat', axis=1).copy()\n",
    "    test_set = test_stratified.drop('revenue_cat', axis=1).copy()\n",
    "    \n",
    "    # Features definieren\n",
    "    numeric_features = ['Year', 'Score', 'Metascore', 'Vote', 'Runtime']\n",
    "    numeric_features = [f for f in numeric_features if f in train_set.columns]\n",
    "    \n",
    "    categorical_features = ['Genre']\n",
    "    categorical_features = [f for f in categorical_features if f in train_set.columns]\n",
    "    \n",
    "    all_features = numeric_features + categorical_features\n",
    "    \n",
    "    # Features und Labels\n",
    "    X_train = train_set[all_features]\n",
    "    y_train = train_set['Revenue']\n",
    "    X_test = test_set[all_features]\n",
    "    y_test = test_set['Revenue']\n",
    "    \n",
    "    print(f\"‚úÖ Training Set: {X_train.shape}\")\n",
    "    print(f\"‚úÖ Test Set: {X_test.shape}\")\n",
    "    print(f\"\\nüìä Features:\")\n",
    "    print(f\"Numerisch: {numeric_features}\")\n",
    "    print(f\"Kategorisch: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline erstellen\n",
    "if 'X_train' in locals():\n",
    "    # Numerische Pipeline\n",
    "    numeric_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Kategorische Pipeline\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Kombinierte Pipeline\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    # Daten transformieren\n",
    "    X_train_prepared = preprocessor.fit_transform(X_train)\n",
    "    X_test_prepared = preprocessor.transform(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessing abgeschlossen\")\n",
    "    print(f\"Transformed shape: {X_train_prepared.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelle trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "if 'X_train_prepared' in locals():\n",
    "    print(\"üìà Training Linear Regression...\")\n",
    "    \n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred_train = lin_reg.predict(X_train_prepared)\n",
    "    y_pred_test = lin_reg.predict(X_test_prepared)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nüìä Linear Regression:\")\n",
    "    print(f\"Train RMSE: {train_rmse:,.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "    print(f\"Test R¬≤: {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "if 'X_train_prepared' in locals():\n",
    "    print(\"üå≤ Training Decision Tree...\")\n",
    "    \n",
    "    tree_reg = DecisionTreeRegressor(random_state=42, max_depth=10)\n",
    "    tree_reg.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred_train = tree_reg.predict(X_train_prepared)\n",
    "    y_pred_test = tree_reg.predict(X_test_prepared)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nüìä Decision Tree:\")\n",
    "    print(f\"Train RMSE: {train_rmse:,.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "    print(f\"Test R¬≤: {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "if 'X_train_prepared' in locals():\n",
    "    print(\"üå≥ Training Random Forest...\")\n",
    "    \n",
    "    forest_reg = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    forest_reg.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred_train = forest_reg.predict(X_train_prepared)\n",
    "    y_pred_test = forest_reg.predict(X_test_prepared)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\nüìä Random Forest:\")\n",
    "    print(f\"Train RMSE: {train_rmse:,.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:,.2f}\")\n",
    "    print(f\"Test R¬≤: {test_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation f√ºr Random Forest\n",
    "if 'forest_reg' in locals():\n",
    "    print(\"üîÑ Performing 5-Fold Cross-Validation...\")\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        forest_reg,\n",
    "        X_train_prepared, y_train,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_rmse = np.sqrt(-cv_scores)\n",
    "    \n",
    "    print(f\"\\nüìä Cross-Validation Results:\")\n",
    "    print(f\"RMSE per fold: {cv_rmse}\")\n",
    "    print(f\"Mean RMSE: {cv_rmse.mean():,.2f}\")\n",
    "    print(f\"Std RMSE: {cv_rmse.std():,.2f}\")\n",
    "    print(f\"95% Confidence: [{cv_rmse.mean() - 2*cv_rmse.std():,.2f}, {cv_rmse.mean() + 2*cv_rmse.std():,.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter-Optimierung mit GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV f√ºr Random Forest\n",
    "if 'X_train_prepared' in locals():\n",
    "    print(\"üîç GridSearchCV f√ºr Random Forest...\")\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    forest_reg_grid = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        forest_reg_grid,\n",
    "        param_grid,\n",
    "        cv=3,  # Weniger Folds f√ºr schnellere Ausf√ºhrung\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Beste Parameter: {grid_search.best_params_}\")\n",
    "    print(f\"Bester CV Score (RMSE): {np.sqrt(-grid_search.best_score_):,.2f}\")\n",
    "    \n",
    "    # Test mit bestem Modell\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_best = best_model.predict(X_test_prepared)\n",
    "    test_rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "    \n",
    "    print(f\"Test RMSE (optimiert): {test_rmse_best:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RandomizedSearchCV (Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV f√ºr schnellere Suche\n",
    "if 'X_train_prepared' in locals():\n",
    "    print(\"üé≤ RandomizedSearchCV f√ºr Random Forest...\")\n",
    "    \n",
    "    param_distributions = {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'max_depth': randint(5, 30),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    }\n",
    "    \n",
    "    forest_reg_random = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        forest_reg_random,\n",
    "        param_distributions,\n",
    "        n_iter=10,  # 10 zuf√§llige Kombinationen\n",
    "        cv=3,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Beste Parameter: {random_search.best_params_}\")\n",
    "    print(f\"Bester CV Score (RMSE): {np.sqrt(-random_search.best_score_):,.2f}\")\n",
    "    \n",
    "    # Test mit bestem Modell\n",
    "    best_model_random = random_search.best_estimator_\n",
    "    y_pred_random = best_model_random.predict(X_test_prepared)\n",
    "    test_rmse_random = np.sqrt(mean_squared_error(y_test, y_pred_random))\n",
    "    \n",
    "    print(f\"Test RMSE (optimiert): {test_rmse_random:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichere bestes Modell und Preprocessor\n",
    "if 'best_model' in locals():\n",
    "    # Erstelle komplette Pipeline\n",
    "    final_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', best_model)\n",
    "    ])\n",
    "    \n",
    "    # Speichern\n",
    "    model_path = 'movies_stratified_model.pkl'\n",
    "    joblib.dump(final_pipeline, model_path)\n",
    "    print(f\"‚úÖ Pipeline gespeichert: {model_path}\")\n",
    "    \n",
    "    # Test: Laden und Vorhersage\n",
    "    loaded_pipeline = joblib.load(model_path)\n",
    "    test_pred = loaded_pipeline.predict(X_test.head(5))\n",
    "    print(f\"\\n‚úÖ Modell erfolgreich geladen und getestet\")\n",
    "    print(f\"Beispiel-Vorhersagen: {test_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zusammenfassung\n",
    "\n",
    "In diesem Notebook haben wir:\n",
    "\n",
    "1. ‚úÖ **Stratified Sampling** angewendet f√ºr ausgewogene Splits\n",
    "2. ‚úÖ Random vs. Stratified Split **verglichen**\n",
    "3. ‚úÖ Moderne **Preprocessing Pipeline** erstellt\n",
    "4. ‚úÖ Mehrere Modelle trainiert (Linear, Decision Tree, Random Forest)\n",
    "5. ‚úÖ **Cross-Validation** durchgef√ºhrt\n",
    "6. ‚úÖ **GridSearchCV** f√ºr Hyperparameter-Optimierung\n",
    "7. ‚úÖ **RandomizedSearchCV** als Alternative\n",
    "8. ‚úÖ Komplette Pipeline gespeichert\n",
    "\n",
    "### Wichtige Aktualisierungen (2025):\n",
    "\n",
    "- ‚úÖ `StratifiedShuffleSplit` mit `random_state`\n",
    "- ‚úÖ `SimpleImputer` statt deprecated `Imputer`\n",
    "- ‚úÖ `OneHotEncoder` mit `handle_unknown='ignore'` und `sparse_output=False`\n",
    "- ‚úÖ `ColumnTransformer` f√ºr klare Feature-Trennung\n",
    "- ‚úÖ `StandardScaler` in Pipeline statt normalize Parameter\n",
    "- ‚úÖ Konsistente `random_state` f√ºr Reproduzierbarkeit\n",
    "- ‚úÖ Moderne Pipeline-Persistenz mit joblib\n",
    "\n",
    "### Warum Stratified Sampling?\n",
    "\n",
    "**Vorteile:**\n",
    "- Repr√§sentative Verteilung in Train/Test\n",
    "- Bessere Generalisierung bei unbalancierten Daten\n",
    "- Zuverl√§ssigere Evaluation\n",
    "\n",
    "**Wann verwenden:**\n",
    "- Bei Klassifikation mit unbalancierten Klassen\n",
    "- Bei Regression mit wichtigen Kategorien\n",
    "- Wenn kleine Subgruppen erhalten bleiben sollen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
